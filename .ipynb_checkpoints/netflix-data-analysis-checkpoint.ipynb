{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow 2.3.1 requires numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#Dependencies, in case they are not already installed in the environment\n",
    "!pip install kaggle --upgrade -q\n",
    "!pip install numpy --upgrade -q\n",
    "!pip install opendatasets --upgrade -q\n",
    "!pip install pandas --upgrade -q\n",
    "!pip install seaborn --upgrade -q\n",
    "!pip install matplotlib --upgrade -q\n",
    "!pip install jovian --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kaggle\n",
    "import jovian\n",
    "from io import StringIO # For creation of string inputs to be read as '.tsv' by pandas\n",
    "import re # For separation of netflix_df.country strings into lists\n",
    "import gzip # For extraction of IMDB datasets\n",
    "import urllib.request\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Cleaning\n",
    "\n",
    "### We will download and combine all datasets necessary to our investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.api.authenticate()\n",
    "kaggle.api.dataset_download_files('shivamb/netflix-shows', path=os.getcwd(), unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'shivamb/netflix-shows' dataset should have downloaded and been extracted to expose file: 'netflix_titles.csv'\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "netflix_df_raw = pd.read_csv('netflix_titles.csv')\n",
    "netflix_df = netflix_df_raw.copy()\n",
    "netflix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Netflix download does not include any information on viewers' ratings; the 'rating' column in the above info refers to MPAA rating.  We can get users' rating info from IMDB.  They make their data available at https://datasets.imdbws.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download IMDB files from official repository\n",
    "urllib.request.urlretrieve('https://datasets.imdbws.com/title.ratings.tsv.gz', 'title.ratings.tsv.gz');\n",
    "urllib.request.urlretrieve('https://datasets.imdbws.com/title.basics.tsv.gz', 'title.basics.tsv.gz');\n",
    "\n",
    "#extract files as text\n",
    "with gzip.open('title.ratings.tsv.gz', 'rt', encoding='utf8') as reader:\n",
    "    ratings_contents = reader.read()\n",
    "\n",
    "with gzip.open('title.basics.tsv.gz', 'rt', encoding='utf8') as reader:\n",
    "    basics_contents = reader.read()\n",
    "    \n",
    "# Create text strings to be loaded into panda.read_csv as a tab separated file\n",
    "ratings_data = StringIO(ratings_contents)\n",
    "basics_data = StringIO(basics_contents)\n",
    "\n",
    "# Create pandas dataframe from tab separated file for ratings\n",
    "imdb_ratings_df_raw = pd.read_csv(ratings_data, sep='\\t')\n",
    "imdb_ratings_df = imdb_ratings_df_raw.copy()\n",
    "\n",
    "# Create pandas dataframe from tab separated file for titles\n",
    "imdb_titles_df_raw = pd.read_csv(basics_data, sep='\\t')\n",
    "imdb_titles_df = imdb_titles_df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the IMDB database is so vast, it comes in multiple files.  I only grabbed the two that I need to make a connection to the Netflix data: 'title.ratings.tsv' and 'title.basics.tsv'.  The tables are linked with a shared key, 'tconst', used to correctly join the data.  The following table includes all titles with any associated ratings from the entire IMDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join btw the two IMDB dataframes\n",
    "imdb_combo_df = pd.merge(imdb_titles_df, imdb_ratings_df, on='tconst')\n",
    "imdb_combo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll save our IMDB dataset as a '.csv' file in case it comes in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_combo_df.to_csv('imdb_combo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_combo_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we want just the titles listed in the Kaggle Netflix dataset downloaded from https://www.kaggle.com/shivamb/netflix-shows which also have rating information on IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join the two datasets with the intention of having only the titles on netflix remain, but include their information from IMDB\n",
    "final_df_raw = pd.merge(netflix_df, imdb_combo_df, left_on='title', right_on='primaryTitle')\n",
    "final_df = final_df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = final_df.groupby('cast')['genres'].apply(','.join).reset_index()\n",
    "final_df.sort_values('cast').genres = temp.genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have some duplicate descriptions because of alternate language versions of some titles, for instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df[netflix_df.title.str.contains('Solo: A Star Wars Story')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(netflix_df.description.iloc[3184] == netflix_df.description.iloc[3185])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few titles had 'NaN' for 'date_added'; notably, the TV show 'Friends'.  Since Friends was such a popular show, I wanted to find a way to include them in the table so I separated them with a dummy date of January 1, 2001.  I also changed 'NaN' values in the 'country' and 'director' columns to the python String 'Unknown' for grouping and comparison purposes; 'NaN' will not evaluate correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.country = final_df.country.fillna('Unknown')\n",
    "final_df.director = final_df.director.fillna('Unknown')\n",
    "final_df.date_added = final_df.date_added.fillna('January 1, 2001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.title.duplicated().sum())\n",
    "print(final_df.description.duplicated().sum())\n",
    "print(final_df.cast.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmm, a number of titles seem to be duplicates...  But we know it is the case that more than one movie can share a title...\n",
    "### We will group the dataframe on 'cast' because there is little chance of accidentally grouping entries together since the 'cast' string, like the 'description' string, is so specific and less likely to appear in multiple entries than, say, 'title'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.groupby('cast').agg({'title':'max', 'type':'max', 'director':'max', 'country':'max', 'date_added':'max', 'description':'max', 'averageRating':'mean', 'numVotes':'sum','genres':'max','listed_in':'max'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This leaves us with 4387 original titles, all on Netflix and all with complete IMDB rating information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.sort_values('numVotes', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're going to parse the 'date_added' column into individual columns and add another column containing the number of countries each title was released to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.date_added = pd.to_datetime(final_df.date_added)\n",
    "#final_df = final_df[final_df.date_added.isna() == False]\n",
    "final_df['year_added'] = pd.DatetimeIndex(final_df.date_added).year.astype(int)\n",
    "final_df['month_added'] = pd.DatetimeIndex(final_df.date_added).month.astype(int)\n",
    "final_df['day_added'] = pd.DatetimeIndex(final_df.date_added).day.astype(int)\n",
    "\n",
    "\n",
    "# Add a numeric column for the number of countries each title was released to\n",
    "myList = []\n",
    "for i in range(final_df.country.size):\n",
    "    myList.append(final_df.country[i].split(','))\n",
    "\n",
    "# Make a pandas Series from 'country' list\n",
    "country = pd.Series(myList)\n",
    "\n",
    "# Create 'country_count' column using map and lambda function involving 'country' Series\n",
    "final_df['country_count'] = country.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above are the mean, standard deviation, min, max and quartiles for numeric columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "matplotlib.rcParams['figure.figsize'] = (9, 5)\n",
    "matplotlib.rcParams['figure.facecolor'] = '#00000000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the number of titles available, where the countries are those with the 10 highest title amounts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xplt = final_df.groupby('country').count().sort_values('title', ascending=False).reset_index().country.head(10)\n",
    "yplt = final_df.groupby('country').count().sort_values('title', ascending=False).reset_index().title.head(10)\n",
    "plt.xticks(rotation=75)\n",
    "plt.plot(xplt, yplt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the distribution of values from 'averageRating', 'year_added' and 'month_added' columns, separated into 'Movies' and 'TV Shows'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_avg = final_df[final_df.type == 'Movie'].averageRating.mean()\n",
    "tv_avg = final_df[final_df.type == 'TV Show'].averageRating.mean()\n",
    "\n",
    "print(movie_avg, tv_avg)\n",
    "\n",
    "matplotlib.rcParams['font.size'] = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "axes[0].hist(final_df.year_added, bins=np.arange(2000,2020,1));\n",
    "axes[0].set_title('Year Added')\n",
    "axes[1].hist(final_df.month_added, bins=np.arange(1,12,1));\n",
    "axes[1].set_title('Month Added')\n",
    "\n",
    "padding=(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The content 'uploaded' in 2001/2002 is the content of unkown date which i segregated as 01/01/2001.  Also, it appears that most content is added during the months of December and January; basically, at the new year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I am isolating the countries with the ultimate goal of getting a more accurate vote count.  Each individual country's totals include the vote counts for any release group that the country was included in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indv_countries = final_df[final_df.country_count == 1].groupby('country')['numVotes'].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary to hold country and vote counts; this will become our dataframe\n",
    "full_votes = {}\n",
    "\n",
    "for country in indv_countries.country:\n",
    "    vote_counter = 0\n",
    "    for entry in final_df.iterrows():\n",
    "        if country in entry[1].country:\n",
    "            vote_counter += entry[1].numVotes\n",
    "    full_votes[country] = vote_counter\n",
    "    \n",
    "# Create a dataframe from the dictionary above\n",
    "full_votes_df = pd.DataFrame.from_dict(full_votes, orient='index', columns=['total']).sort_values('total', ascending=False).reset_index()\n",
    "full_votes_df = full_votes_df.rename(({'index':'country'}), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matplotlib.rcParams['font.size'] = 100\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('Number of Votes, by Country (including US)')\n",
    "plt.xlabel('Votes', fontsize=20)\n",
    "plt.ylabel('Country', fontsize=20)\n",
    "sns.barplot(x=full_votes_df.total.head(10), y=full_votes_df.country.head(10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('Number of Votes, by Country')\n",
    "sns.barplot(x=full_votes_df.total.iloc[1:21], y=full_votes_df.country.iloc[1:21]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('Number of Votes, by Country')\n",
    "sns.barplot(x=full_votes_df.total.iloc[2:21], y=full_votes_df.country.iloc[2:21]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('The 20 Countries with the Lowest Vote Count')\n",
    "sns.barplot(x=full_votes_df.total.iloc[-20:-1], y=full_votes_df.country.iloc[-20:-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see where the ratings are clustered by plotting value density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Ratings for United States, United Kingdom and India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(final_df[(final_df.country == 'United States') |\n",
    "                     (final_df.country == 'United Kingdom') |\n",
    "                     (final_df.country == 'India')].averageRating,\n",
    "            hue=final_df[(final_df.country == 'United States') |\n",
    "                                   (final_df.country == 'United Kingdom') |\n",
    "                                   (final_df.country == 'India')].country);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a dataframe containing some countries we are interested in, not including 'United States' or other major outliers which can make the graph difficult to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_density_list = ['Germany', 'Canada', 'France', 'Japan',\n",
    "                        'Australia', 'New Zealand', 'China']\n",
    "\n",
    "country_density_df = pd.DataFrame()\n",
    "\n",
    "for country in country_density_list:\n",
    "    country_density_df = pd.concat([country_density_df, final_df[final_df.country == country]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = widgets.SelectMultiple(\n",
    "    options=list(final_df.country),\n",
    "    value=[],\n",
    "    #rows=10,\n",
    "    description='Countries',\n",
    "    disabled=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see how average ratings vary across countries.  Might there be a cultural component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(country_density_df.averageRating, hue=country_density_df.country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.director.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the ten most commonly used words throughout all of the descriptions, many of them unsurprising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list = []\n",
    "for desc in netflix_df.description:\n",
    "    desc = desc.lower()\n",
    "    desc_list.append(desc.replace('_', ' ').replace(', ', ' ').replace('.', ' ').replace('-', ' ').split())\n",
    "    \n",
    "desc_word_list = []\n",
    "for desc in desc_list:\n",
    "    for word in desc:\n",
    "        desc_word_list.append(word.strip())\n",
    "        \n",
    "desc_word_pd = pd.DataFrame(desc_word_list, columns=['word'])\n",
    "        \n",
    "desc_words_df = pd.DataFrame(desc_word_list, columns=['word'])\n",
    "desc_words_grp = pd.DataFrame(desc_words_df.groupby('word')['word'].count())\n",
    "desc_words_grp = desc_words_grp.rename(columns={'word':'frq'})\n",
    "\n",
    "top_words = pd.DataFrame(desc_words_grp.frq.sort_values(ascending=False).head(1000), columns=['frq']).reset_index() # reset_index() allows us to create a new index beginning from 0 and, ultimately, more easily drop unneccessary columns\n",
    "\n",
    "top_words['percentage'] = top_words.frq.map(lambda x: (x / top_words.frq.sum()) * 100)\n",
    "\n",
    "top_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The pie chart below displays the frequency of all words across all descriptions.  It gets far too illegible; let's break it down further, because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 15\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.pie(top_words.frq, labels=top_words.word, autopct='%1.1f%%', startangle=180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...the top ten words alone make up over 35% of the word usage; of that 35%, the word 'a' makes up over a fifth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words.percentage.head(10).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.fontsize=15\n",
    "plt.pie(top_words.frq.head(10), labels=top_words.word.head(10), autopct='%1.1f%%', startangle=180);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency for US in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_us_df = netflix_df[netflix_df.country == 'United States']\n",
    "\n",
    "desc_list2 = []\n",
    "for desc in netflix_us_df.description:\n",
    "    desc = desc.lower()\n",
    "    desc_list2.append(desc.replace('_', ' ').replace(', ', ' ').replace('.', ' ').replace('-', ' ').split())\n",
    "    \n",
    "desc_word_list2 = []\n",
    "for desc in desc_list2:\n",
    "    for word in desc:\n",
    "        desc_word_list2.append(word.strip())\n",
    "        \n",
    "desc_word_pd2 = pd.DataFrame(desc_word_list2, columns=['word'])\n",
    "        \n",
    "desc_words_df2 = pd.DataFrame(desc_word_list2, columns=['word'])\n",
    "desc_words_grp2 = pd.DataFrame(desc_words_df2.groupby('word')['word'].count())\n",
    "desc_words_grp2 = desc_words_grp2.rename(columns={'word':'frq'})\n",
    "\n",
    "top_words_us = pd.DataFrame(desc_words_grp2.frq.sort_values(ascending=False).head(1000), columns=['frq'])\n",
    "top_words_us = top_words_us.reset_index() # reset_index() allows us to create a new index beginning from 0 and, ultimately, more easily drop unneccessary columns\n",
    "\n",
    "top_words_us['percentage'] = top_words_us.frq.map(lambda x: (x / top_words_us.frq.sum()) * 100)\n",
    "top_words_us['country'] = 'United States'\n",
    "\n",
    "top_words_us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency for India, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_india_df = netflix_df[netflix_df.country == 'India']\n",
    "\n",
    "desc_list3 = []\n",
    "for desc in netflix_india_df.description:\n",
    "    desc = desc.lower()\n",
    "    desc_list3.append(desc.replace('_', ' ').replace(', ', ' ').replace('.', ' ').replace('-', ' ').split())\n",
    "    \n",
    "desc_word_list3 = []\n",
    "for desc in desc_list3:\n",
    "    for word in desc:\n",
    "        desc_word_list3.append(word.strip())\n",
    "        \n",
    "desc_word_pd3 = pd.DataFrame(desc_word_list3, columns=['word'])\n",
    "        \n",
    "desc_words_df3 = pd.DataFrame(desc_word_list3, columns=['word'])\n",
    "desc_words_grp3 = pd.DataFrame(desc_words_df3.groupby('word')['word'].count())\n",
    "desc_words_grp3 = desc_words_grp3.rename(columns={'word':'frq'})\n",
    "\n",
    "top_words_india = pd.DataFrame(desc_words_grp3.frq.sort_values(ascending=False).head(1000), columns=['frq'])\n",
    "top_words_india = top_words_india.reset_index() # reset_index() allows us to create a new index beginning from 0 and, ultimately, more easily drop unneccessary columns\n",
    "\n",
    "top_words_india['percentage'] = top_words_india.frq.map(lambda x: (x / top_words_us.frq.sum()) * 100)\n",
    "top_words_india['country'] = 'India'\n",
    "\n",
    "top_words_india"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency for UK, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_uk_df = netflix_df[netflix_df.country == 'United Kingdom']\n",
    "\n",
    "desc_list4 = []\n",
    "for desc in netflix_uk_df.description:\n",
    "    desc = desc.lower()\n",
    "    desc_list4.append(desc.replace('_', ' ').replace(', ', ' ').replace('.', ' ').replace('-', ' ').split())\n",
    "    \n",
    "desc_word_list4 = []\n",
    "for desc in desc_list4:\n",
    "    for word in desc:\n",
    "        desc_word_list4.append(word.strip())\n",
    "        \n",
    "desc_word_pd4 = pd.DataFrame(desc_word_list4, columns=['word'])\n",
    "        \n",
    "desc_words_df4 = pd.DataFrame(desc_word_list4, columns=['word'])\n",
    "desc_words_grp4 = pd.DataFrame(desc_words_df4.groupby('word')['word'].count())\n",
    "desc_words_grp4 = desc_words_grp4.rename(columns={'word':'frq'})\n",
    "\n",
    "top_words_uk = pd.DataFrame(desc_words_grp4.frq.sort_values(ascending=False).head(1000), columns=['frq'])\n",
    "top_words_uk = top_words_uk.reset_index() # reset_index() allows us to create a new index beginning from 0 and, ultimately, more easily drop unneccessary columns\n",
    "\n",
    "top_words_uk['percentage'] = top_words_uk.frq.map(lambda x: (x / top_words_us.frq.sum()) * 100)\n",
    "top_words_uk['country'] = 'United Kingdom'\n",
    "\n",
    "top_words_uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The top five words for all the countries are identical, except for their position on the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies_added = no_2001[no_2001.type == 'Movie'].groupby('year_added')['title'].count()\n",
    "#movies_added\n",
    "\n",
    "#tv_added = no_2001[no_2001.type == 'TV Show'].groupby('year_added')['title'].count()\n",
    "#tv_added\n",
    "\n",
    "#no_2001 = final_df[final_df.year_added != 2001]\n",
    "#years = sorted(no_2001.year_added.unique())\n",
    "#tv = tv_added.to_list()\n",
    "#movies = movies_added.to_list()\n",
    "#print(years, tv, movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(years, tv, 'b-x', linewidth=4, markersize=12, markeredgewidth=4, markeredgecolor='navy');\n",
    "#plt.plot(years, movies, 'r--o', linewidth=4, markersize=12);\n",
    "#plt.title('Movies and TV Shows added to Netflix By Year')\n",
    "#plt.legend(['Movies', 'TV Shows'])\n",
    "#plt.xlabel('Year'); plt.ylabel('Titles Added');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at some of the least common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(64, 32))\n",
    "matplotlib.rcParams['font.size'] = 50\n",
    "axes[0].pie(top_words_us.frq.tail(20), labels=top_words_us.word.tail(20), autopct='%1.1f%%', startangle=180);\n",
    "axes[0].set_title('United States')\n",
    "axes[1].pie(top_words_india.frq.tail(20), labels=top_words_india.word.tail(20), autopct='%1.1f%%', startangle=180);\n",
    "axes[1].set_title('India')\n",
    "axes[2].pie(top_words_uk.frq.tail(20), labels=top_words_uk.word.tail(20), autopct='%1.1f%%', startangle=180);\n",
    "axes[2].set_title('United Kingdom')\n",
    "\n",
    "plt.tight_layout(pad=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above pie charts show the 20 least common descriptive words for US, India and UK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is a scatterplot of 'averageRating' VS 'numVotes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 10\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.scatterplot(final_df[(final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India')].sort_values('averageRating', ascending=False).averageRating, final_df[(final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India')].sort_values('averageRating', ascending=False).numVotes, hue=final_df[(final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India')].sort_values('averageRating', ascending=False).country.tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 10\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.scatterplot(final_df[(final_df.averageRating >= 5) & ((final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India'))].sort_values('averageRating', ascending=False).averageRating, final_df[(final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India')].sort_values('averageRating', ascending=False).numVotes, hue=final_df[(final_df.country == 'United States') |\n",
    "                         (final_df.country == 'United Kingdom') |\n",
    "                         (final_df.country == 'India')].sort_values('averageRating', ascending=False).country.tolist());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking and Answering Questions\n",
    "\n",
    "### Explore differences between, as well as similarities among, content available in different countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What are the least common words used in description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_list = []\n",
    "for desc in netflix_df.description:\n",
    "    desc = desc.lower()\n",
    "    desc_list.append(desc.replace('_', ' ').replace(', ', ' ').replace('.', ' ').replace('-', ' ').split())\n",
    "    \n",
    "desc_word_list = []\n",
    "for desc in desc_list:\n",
    "    for word in desc:\n",
    "        desc_word_list.append(word.strip())\n",
    "        \n",
    "desc_word_pd = pd.DataFrame(desc_word_list, columns=['word'])\n",
    "        \n",
    "desc_words_df = pd.DataFrame(desc_word_list, columns=['word'])\n",
    "desc_words_grp = pd.DataFrame(desc_words_df.groupby('word')['word'].count())\n",
    "desc_words_grp = desc_words_grp.rename(columns={'word':'frq'})\n",
    "\n",
    "top_words = pd.DataFrame(desc_words_grp.frq.sort_values(ascending=False).head(1000), columns=['frq']).reset_index() # reset_index() allows us to create a new index beginning from 0 and, ultimately, more easily drop unneccessary columns\n",
    "\n",
    "top_words['percentage'] = top_words.frq.map(lambda x: (x / top_words.frq.sum()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_words = top_words[top_words.frq <= 50].sample(100).tail(10)[['word', 'frq', 'percentage']].reset_index().drop('index', axis=1)\n",
    "uc_words.sort_values('frq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we have a list of 10 of the LEAST commonly used words, generated by taking the tail of a random sample of 100 of the words whose frequency is 75 or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Which country consistently gives the highest ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_ratings = final_df[final_df.country == 'United States'].averageRating\n",
    "uk_ratings = final_df[final_df.country == 'United Kingdom'].averageRating\n",
    "india_ratings = final_df[final_df.country == 'India'].averageRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([us_ratings, uk_ratings, india_ratings], \n",
    "         stacked=True);\n",
    "plt.legend(['United States', 'United Kingdom', 'India']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Because India consistently gives the highest ratings to its available content, which type of content is preferred in India: Movie or TV Show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_rating_tv = final_df[(final_df.country == 'India')].groupby('type')['averageRating'].mean()\n",
    "india_rating_movie = final_df[(final_df.country == 'India') & (final_df.type == 'Movie')]['averageRating'].mean()\n",
    "\n",
    "plt.hist([india_rating_tv, india_rating_movie]);\n",
    "\n",
    "plt.legend(['Movie', 'TV Show']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It appears that India, as a country, prefers the TV Shows on Netflix over the Movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Does Netflix availability follow the Pareto Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the Pareto Distribution to hold for overall access to content by country, the top 20% of the countries in the list of content amount should account for 80% of the overall usage.  Let's see if this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 15\n",
    "\n",
    "xplt = final_df.groupby('country').count().sort_values('title', ascending=False).reset_index().country.head(10)\n",
    "yplt = final_df.groupby('country').count().sort_values('title', ascending=False).reset_index().title.head(10)\n",
    "plt.xticks(rotation=75)\n",
    "plt.plot(xplt, yplt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This graph closely mirrors a Pareto Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are obviously many interrelations between the data in this set, however, it appears that a prime takeaway is that TV Shows are valued more than movies in today's world;  in addition, it appears that India is a fast growing market which uses the rating system more favorably than some other, Western countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.kaggle.com/shivamb/netflix-shows\n",
    "* https://datasets.imdbws.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There should be more care taken when comparing titles by country.  There are a number of titles which are available in multiple countries and these combinations are counted as their own 'country'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade -q\n",
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\r\n"
     ]
    }
   ],
   "source": [
    "jovian.commit(project_name=\"Netflix-Data-Analysis\", files=['imdb_combo.csv', 'netflix_titles.csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
